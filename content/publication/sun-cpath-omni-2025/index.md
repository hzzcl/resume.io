---
title: 'CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide
  Image Analysis in Computational Pathology'
authors:
- Yuxuan Sun
- Yixuan Si
- Chenglu Zhu
- Xuan Gong
- Kai Zhang
- Pingyi Chen
- Ye Zhang
- Zhongyi Shui
- Tao Lin
- Lin Yang
date: '2025-06-01'
publishDate: '2026-01-30T15:23:21.156476Z'
publication_types:
- paper-conference
publication: '*2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR)*'
doi: 10.1109/CVPR52734.2025.00969
abstract: The emergence of large multimodal models (LMMs) has brought significant
  advancements to pathology. Previous research has primarily focused on separately
  training patch-level and whole-slide image (WSI)-level models, limiting the integration
  of learned knowledge across patches and WSIs and resulting in redundant models.
  In this work, we introduce CPath-Omni, the first 15B parameter LMM that unifies
  patch and WSI analysis, consolidating a variety of tasks at both levels, including
  classification, visual question answering, captioning, and visual referring prompting.
  Extensive experiments demonstrate that CPathOmni achieves state-of-the-art (SOTA)
  performance across seven diverse tasks on 39 out of 42 datasets, outperforming or
  matching task-specific models trained for individual tasks. Additionally, we develop
  a specialized pathology CLIP-based visual processor for CPath-Omni, CPath-CLIP,
  which, for the first time, integrates different vision models and incorporates a
  large language model as a text encoder to build a more powerful CLIP model, which
  achieves SOTA performance on nine zero-shot and four few-shot datasets. Our findings
  highlight CPath-Omniâ€™s ability to unify diverse pathology tasks, demonstrating its
  potential to streamline and advance the field of foundation model in pathology.
  The code and model are available at CPath-Omni.
links:
- name: URL
  url: https://ieeexplore.ieee.org/document/11094106/
---
