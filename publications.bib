@inproceedings{jingxiongliPathUpPatchwiseTimestep2024,
  title = {{{PathUp}}: {{Patch-wise Timestep Tracking}} for {{Multi-class Large Pathology Image Synthesising Diffusion Model}}},
  shorttitle = {{{PathUp}}},
  booktitle = {Proceedings of the 32nd {{ACM International Conference}} on {{Multimedia}}},
  author = {{Jingxiong Li} and {Sunyi Zheng} and {Chenglu Zhu} and {Yuxuan Sun} and {Pingyi Chen} and {Zhongyi Shui} and {Yunlong Zhang} and {Honglin Li} and {Lin Yang}},
  date = {2024-10-28},
  pages = {3984--3993},
  publisher = {ACM},
  location = {Melbourne VIC Australia},
  doi = {10.1145/3664647.3681295},
  url = {https://dl.acm.org/doi/10.1145/3664647.3681295},
  urldate = {2024-12-26},
  eventtitle = {{{MM}} '24: {{The}} 32nd {{ACM International Conference}} on {{Multimedia}}},
  isbn = {979-8-4007-0686-8},
  langid = {english},
}

@inproceedings{NEURIPS2024_b7eecb72,
  title = {Rethinking Transformer for Long Contextual Histopathology Whole Slide Image Analysis},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Li, Honglin and Zhang, Yunlong and Chen, Pingyi and Shui, Zhongyi and Zhu, Chenglu and Yang, Lin},
  editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
  date = {2024},
  volume = {37},
  pages = {101498--101528},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/b7eecb72574b043ad0c69ea296212450-Paper-Conference.pdf}
}

@incollection{pingyichenWsiCaptionMultipleInstance2024,
  title = {{{WsiCaption}}: {{Multiple Instance Generation}} of {{Pathology Reports}} for {{Gigapixel Whole-Slide Images}}},
  shorttitle = {{{WsiCaption}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} – {{MICCAI}} 2024},
  author = {{Pingyi Chen} and {Honglin Li} and {Chenglu Zhu} and {Sunyi Zheng} and {Zhongyi Shui} and {Lin Yang}},
  editor = {Linguraru, Marius George and Dou, Qi and Feragen, Aasa and Giannarou, Stamatia and Glocker, Ben and Lekadir, Karim and Schnabel, Julia A.},
  date = {2024},
  volume = {15004},
  pages = {546--556},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-72083-3_51},
  url = {https://link.springer.com/10.1007/978-3-031-72083-3_51},
  urldate = {2024-12-26},
  isbn = {978-3-031-72082-6 978-3-031-72083-3},
  langid = {english},
}

@incollection{pingyichenWSIVQAInterpretingWhole2025,
  title = {{{WSI-VQA}}: {{Interpreting Whole Slide Images}} by {{Generative Visual Question Answering}}},
  shorttitle = {{{WSI-VQA}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2024},
  author = {{Pingyi Chen} and {Chenglu Zhu} and {Sunyi Zheng} and {Honglin Li} and {Lin Yang}},
  editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
  date = {2025},
  volume = {15094},
  pages = {401--417},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-72764-1_23},
  url = {https://link.springer.com/10.1007/978-3-031-72764-1_23},
  urldate = {2024-12-26},
  isbn = {978-3-031-72763-4 978-3-031-72764-1},
  langid = {english},
}

@article{shuiDPAP2PNetDeformableProposalAware2024,
  title = {{{DPA-P2PNet}}: {{Deformable Proposal-Aware P2PNet}} for {{Accurate Point-Based Cell Detection}}},
  shorttitle = {{{DPA-P2PNet}}},
  author = {Shui, Zhongyi and Zheng, Sunyi and Zhu, Chenglu and Zhang, Shichuan and Yu, Xiaoxuan and Li, Honglin and Li, Jingxiong and Chen, Pingyi and Yang, Lin},
  date = {2024-03-24},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {38},
  number = {5},
  pages = {4864--4872},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v38i5.28289},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/28289},
  urldate = {2024-12-26},
  abstract = {Point-based cell detection (PCD), which pursues high-performance cell sensing under low-cost data annotation, has garnered increased attention in computational pathology community. Unlike mainstream PCD methods that rely on intermediate density map representations, the Point-to-Point network (P2PNet) has recently emerged as an end-to-end solution for PCD, demonstrating impressive cell detection accuracy and efficiency. Nevertheless, P2PNet is limited to decoding from a single-level feature map due to the scale-agnostic property of point proposals, which is insufficient to leverage multi-scale information. Moreover, the spatial distribution of pre-set point proposals is biased from that of cells, leading to inaccurate cell localization. To lift these limitations, we present DPA-P2PNet in this work. The proposed method directly extracts multi-scale features for decoding according to the coordinates of point proposals on hierarchical feature maps. On this basis, we further devise deformable point proposals to mitigate the positional bias between proposals and potential cells to promote cell localization. Inspired by practical pathological diagnosis that usually combines high-level tissue structure and low-level cell morphology for accurate cell classification, we propose a multi-field-of-view (mFoV) variant of DPA-P2PNet to accommodate additional large FoV images with tissue information as model input. Finally, we execute the first self-supervised pre-training on immunohistochemistry histopathology image data and evaluate the suitability of four representative self-supervised methods on the PCD task. Experimental results on three benchmarks and a large-scale and real-world interval dataset demonstrate the superiority of our proposed models over the state-of-the-art counterparts. Codes and pre-trained weights are available at https://github.com/windygoo/DPA-P2PNet.},
}

@incollection{yunlongzhangAttentionChallengingMultipleInstance2025,
  title = {Attention-{{Challenging Multiple Instance Learning}} for {{Whole Slide Image Classification}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2024},
  author = {{Yunlong Zhang} and {Honglin Li} and {Yunxuan Sun} and {Sunyi Zheng} and {Chenglu Zhu} and {Lin Yang}},
  editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
  date = {2025},
  volume = {15111},
  pages = {125--143},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-73668-1_8},
  url = {https://link.springer.com/10.1007/978-3-031-73668-1_8},
  urldate = {2024-12-26},
  isbn = {978-3-031-73667-4 978-3-031-73668-1},
  langid = {english},
}

@incollection{yuxuansunPathMMUMassiveMultimodal2025,
  title = {{{PathMMU}}: {{A Massive Multimodal Expert-Level Benchmark}} for {{Understanding}} and {{Reasoning}} in {{Pathology}}},
  shorttitle = {{{PathMMU}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2024},
  author = {{Yuxuan Sun} and {Hao Wu} and {Chenglu Zhu} and {Sunyi Zheng} and {Qizi Chen} and {Kai Zhang} and {Yunlong Zhang} and {Dan Wan} and {Xiaoxiao Lan} and {Mengyue Zheng} and {Jingxiong Li} and {Xinheng Lyu} and {Tao Lin} and {Lin Yang}},
  editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
  date = {2025},
  volume = {15120},
  pages = {56--73},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-73033-7_4},
  url = {https://link.springer.com/10.1007/978-3-031-73033-7_4},
  urldate = {2024-12-26},
  isbn = {978-3-031-73032-0 978-3-031-73033-7},
  langid = {english},
}

@incollection{zhongyishuiUnleashingPowerPromptDriven2025,
  title = {Unleashing the {{Power}} of {{Prompt-Driven Nucleus Instance Segmentation}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2024},
  author = {{Zhongyi Shui} and {Yunlong Zhang} and {Kai Yao} and {Chenglu Zhu} and {Sunyi Zheng} and {Jingxiong Li} and {Honglin Li} and {Yuxuan Sun} and {Ruizhe Guo} and {Lin Yang}},
  editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
  date = {2025},
  volume = {15085},
  pages = {288--304},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-73383-3_17},
  url = {https://link.springer.com/10.1007/978-3-031-73383-3_17},
  urldate = {2024-12-26},
  isbn = {978-3-031-73382-6 978-3-031-73383-3},
  langid = {english},
}


@article{li2025topofm,
  title={ToPoFM: Topology-Guided Pathology Foundation Model for High-Resolution Pathology Image Synthesis with Cellular-Level Control},
  author={Li, Jingxiong and Zhu, Chenglu and Zheng, Sunyi and Chen, Pingyi and Sun, Yuxuan and Li, Honglin and Yang, Lin},
  journal={IEEE Transactions on Medical Imaging},
  year={2025},
  publisher={IEEE}
}

@incollection{sun2024context,
  title={Context-Aware Text-Assisted Multimodal Framework for Cervical Cytology Cell Diagnosis and Chatting},
  author={Sun, Yuxuan and Zhu, Chenglu and Zheng, Sunyi and Zhang, Yunlong and Li, Honglin and Yang, Lin},
  booktitle={2024 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={1--6},
  year={2024},
  organization={IEEE}
}

@article{zhu2021cascaded,
  title={Cascaded residual U-net for fully automatic segmentation of 3D carotid artery in high-resolution multi-contrast MR images},
  author={Zhu, Chenglu and Wang, Xiaoyan and Teng, Zhongzhao and Chen, Shengyong and Huang, Xiaojie and Xia, Ming and Mao, Lizhao and Bai, Cong},
  journal={Physics in Medicine \& Biology},
  volume={66},
  number={4},
  pages={045033},
  year={2021},
  publisher={IOP Publishing}
}

@article{zhu2018automatic,
  title={Automatic centerline extraction of cerebrovascular in 4D CTA based on tubular features},
  author={Zhu, Chenglu and Wang, Xiaoyan and Chen, Shengyong and Xia, Ming and Huang, Yujiao and Pan, Xiang},
  journal={Physics in Medicine \& Biology},
  volume={63},
  number={12},
  pages={125014},
  year={2018},
  publisher={IOP Publishing}
}

@inproceedings{cai2021generalizing,
  title={Generalizing nucleus recognition model in multi-source ki67 immunohistochemistry stained images via domain-specific pruning},
  author={Cai, Jiatong and Zhu, Chenglu and Cui, Can and Li, Honglin and Wu, Tong and Zhang, Shichuan and Yang, Lin},
  booktitle={Medical Image Computing and Computer Assisted Intervention--MICCAI 2021: 24th International Conference, Strasbourg, France, September 27--October 1, 2021, Proceedings, Part VIII 24},
  pages={277--287},
  year={2021},
  organization={Springer International Publishing}
}

@article{wang2022automatic,
  title={Automatic and accurate segmentation of peripherally inserted central catheter (PICC) from chest X-rays using multi-stage attention-guided learning},
  author={Wang, Xiaoyan and Wang, Luyao and Sheng, Ye and Zhu, Chenglu and Jiang, Nan and Bai, Cong and Xia, Ming and Shao, Zhanpeng and Gu, Zheng and Huang, Xiaojie and others},
  journal={Neurocomputing},
  volume={482},
  pages={82--97},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{zhang2022weakly,
  title={Weakly supervised learning for cell recognition in immunohistochemical cytoplasm staining images},
  author={Zhang, Shichuan and Zhu, Chenglu and Li, Honglin and Cai, Jiatong and Yang, Lin},
  booktitle={2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)},
  pages={1--5},
  year={2022},
  organization={IEEE}
}

@inproceedings{zhu2022weakly,
  title={Weakly Supervised Classification using Multi-Level Instance-Aware Optimization on Cervical Cytologic Image},
  author={Zhu, Chenglu and Sun, Yuxuan and Li, Honglin and Cui, Can and Zhang, Shichuan and Cai, Jiatong and Ling, Yang},
  booktitle={2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)},
  pages={1--5},
  year={2022},
  organization={IEEE}
}

@inproceedings{cai2022category,
  title={Category Separation For Weakly Supervised Multi-Class Cell Counting},
  author={Cai, Jiatong and Zhu, Chenglu and Chen, Pingyi and Zhang, Shichuan and Li, Honglin and Sun, Yuxuan and Yang, Lin},
  booktitle={2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)},
  pages={1--5},
  year={2022},
  organization={IEEE}
}

@inproceedings{zheng2022chrsnet,
  title={Chrsnet: Chromosome straightening using self-attention guided networks},
  author={Zheng, Sunyi and Li, Jingxiong and Shui, Zhongyi and Zhu, Chenglu and Zhang, Yunlong and Chen, Pingyi and Yang, Lin},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={119--128},
  year={2022},
  organization={Springer Nature Switzerland Cham}
}

@inproceedings{zhang2022benchmarking,
  title={Benchmarking the robustness of deep neural networks to common corruptions in digital pathology},
  author={Zhang, Yunlong and Sun, Yuxuan and Li, Honglin and Zheng, Sunyi and Zhu, Chenglu and Yang, Lin},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={242--252},
  year={2022},
  organization={Springer Nature Switzerland Cham}
}

@inproceedings{shui2022end,
  title={End-to-end cell recognition by point annotation},
  author={Shui, Zhongyi and Zhang, Shichuan and Zhu, Chenglu and Wang, Bingchuan and Chen, Pingyi and Zheng, Sunyi and Yang, Lin},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={109--118},
  year={2022},
  organization={Springer Nature Switzerland Cham}
}

@article{zhu2022complex,
  title={Complex carotid artery segmentation in multi-contrast MR sequences by improved optimal surface graph cuts based on flow line learning},
  author={Zhu, Chenglu and Wang, Xiaoyan and Chen, Shengyong and Teng, Zhongzhao and Bai, Cong and Huang, Xiaojie and Xia, Ming and Shao, Zhanpeng and Gu, Zheng and Sun, Peiliang},
  journal={Medical \& Biological Engineering \& Computing},
  volume={60},
  number={9},
  pages={2693--2706},
  year={2022},
  publisher={Springer Berlin Heidelberg}
}

@inproceedings{li2023task,
  title={Task-specific fine-tuning via variational information bottleneck for weakly-supervised pathology whole slide image classification},
  author={Li, Honglin and Zhu, Chenglu and Zhang, Yunlong and Sun, Yuxuan and Shui, Zhongyi and Kuang, Wenwei and Zheng, Sunyi and Yang, Lin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7454--7463},
  year={2023}
}

@inproceedings{chen2023exploring,
  title={Exploring Unsupervised Cell Recognition with Prior Self-activation Maps},
  author={Chen, Pingyi and Zhu, Chenglu and Shui, Zhongyi and Cai, Jiatong and Zheng, Sunyi and Zhang, Shichuan and Yang, Lin},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={559--568},
  year={2023},
  organization={Springer Nature Switzerland Cham}
}

@article{zhang2024gradient,
  title={Gradient-aware learning for joint biases: Label noise and class imbalance},
  author={Zhang, Shichuan and Zhu, Chenglu and Li, Honglin and Cai, Jiatong and Yang, Lin},
  journal={Neural Networks},
  volume={171},
  pages={374--382},
  year={2024},
  publisher={Pergamon}
}

@inproceedings{sun2023assessing,
  title={Assessing the Robustness of Deep Learning-Assisted Pathological Image Analysis Under Practical Variables of Imaging System},
  author={Sun, Yuxuan and Zhu, Chenglu and Zhang, Yunlong and Li, Honglin and Chen, Pingyi and Yang, Lin},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@inproceedings{sun2024pathasst,
  title={Pathasst: A generative foundation ai assistant towards artificial general intelligence of pathology},
  author={Sun, Yuxuan and Zhu, Chenglu and Zheng, Sunyi and Zhang, Kai and Sun, Lin and Shui, Zhongyi and Zhang, Yunlong and Li, Honglin and Yang, Lin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={5},
  pages={5034--5042},
  year={2024}
}
@article{sun_pathbench_2025,
	title = {{PathBench}: {Advancing} the {Benchmark} of {Large} {Multimodal} {Models} for {Pathology} {Image} {Understanding} at {Patch} and {Whole} {Slide} {Level}},
	volume = {44},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0278-0062, 1558-254X},
	shorttitle = {{PathBench}},
	url = {https://ieeexplore.ieee.org/document/11062674/},
	doi = {10.1109/TMI.2025.3584857},
	language = {en},
	number = {10},
	urldate = {2025-12-29},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Sun, Yuxuan and Wu, Hao and Zhu, Chenglu and Si, Yixuan and Chen, Qizi and Zhang, Yunlong and Zhang, Kai and Li, Jingxiong and Cai, Jiatong and Wang, Yuhan and Sun, Lin and Lin, Tao and Yang, Lin},
	month = oct,
	year = {2025},
	pages = {4087--4097},
}

@misc{sun_cpathagent_2025,
	title = {{CPathAgent}: {An} {Agent}-based {Foundation} {Model} for {Interpretable} {High}-{Resolution} {Pathology} {Image} {Analysis} {Mimicking} {Pathologists}' {Diagnostic} {Logic}},
	shorttitle = {{CPathAgent}},
	url = {http://arxiv.org/abs/2505.20510},
	doi = {10.48550/arXiv.2505.20510},
	abstract = {Recent advances in computational pathology have led to the emergence of numerous foundation models. These models typically rely on general-purpose encoders with multi-instance learning for whole slide image (WSI) classification or apply multimodal approaches to generate reports directly from images. However, these models cannot emulate the diagnostic approach of pathologists, who systematically examine slides at low magnification to obtain an overview before progressively zooming in on suspicious regions to formulate comprehensive diagnoses. Instead, existing models directly output final diagnoses without revealing the underlying reasoning process. To address this gap, we introduce CPathAgent, an innovative agent-based approach that mimics pathologists’ diagnostic workflow by autonomously navigating across WSI through zoom-in/out and move operations based on observed visual features, thereby generating substantially more transparent and interpretable diagnostic summaries. To achieve this, we develop a multi-stage training strategy that unifies patch-level, region-level, and WSI-level capabilities within a single model, which is essential for replicating how pathologists understand and reason across diverse image scales. Additionally, we construct PathMMU-HR², the first expert-validated benchmark for large region analysis. This represents a critical intermediate scale between patches and whole slides, reflecting a key clinical reality where pathologists typically examine several key large regions rather than entire slides at once. Extensive experiments demonstrate that CPathAgent consistently outperforms existing approaches across benchmarks at three different image scales, validating the effectiveness of our agent-based diagnostic approach and highlighting a promising direction for computational pathology.},
	language = {en},
	urldate = {2025-12-29},
	publisher = {arXiv},
	author = {Sun, Yuxuan and Si, Yixuan and Zhu, Chenglu and Zhang, Kai and Shui, Zhongyi and Ding, Bowen and Lin, Tao and Yang, Lin},
	month = oct,
	year = {2025},
	note = {arXiv:2505.20510 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{sun_cpath-omni_2025,
	address = {Nashville, TN, USA},
	title = {{CPath}-{Omni}: {A} {Unified} {Multimodal} {Foundation} {Model} for {Patch} and {Whole} {Slide} {Image} {Analysis} in {Computational} {Pathology}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3315-4364-8},
	shorttitle = {{CPath}-{Omni}},
	url = {https://ieeexplore.ieee.org/document/11094106/},
	doi = {10.1109/CVPR52734.2025.00969},
	abstract = {The emergence of large multimodal models (LMMs) has brought significant advancements to pathology. Previous research has primarily focused on separately training patch-level and whole-slide image (WSI)-level models, limiting the integration of learned knowledge across patches and WSIs and resulting in redundant models. In this work, we introduce CPath-Omni, the first 15B parameter LMM that unifies patch and WSI analysis, consolidating a variety of tasks at both levels, including classification, visual question answering, captioning, and visual referring prompting. Extensive experiments demonstrate that CPathOmni achieves state-of-the-art (SOTA) performance across seven diverse tasks on 39 out of 42 datasets, outperforming or matching task-specific models trained for individual tasks. Additionally, we develop a specialized pathology CLIP-based visual processor for CPath-Omni, CPath-CLIP, which, for the first time, integrates different vision models and incorporates a large language model as a text encoder to build a more powerful CLIP model, which achieves SOTA performance on nine zero-shot and four few-shot datasets. Our findings highlight CPath-Omni’s ability to unify diverse pathology tasks, demonstrating its potential to streamline and advance the field of foundation model in pathology. The code and model are available at CPath-Omni.},
	language = {en},
	urldate = {2025-12-29},
	booktitle = {2025 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Sun, Yuxuan and Si, Yixuan and Zhu, Chenglu and Gong, Xuan and Zhang, Kai and Chen, Pingyi and Zhang, Ye and Shui, Zhongyi and Lin, Tao and Yang, Lin},
	month = jun,
	year = {2025},
	pages = {10360--10371},
}

@misc{li_pathvq_2025,
	title = {{PathVQ}: {Reforming} {Computational} {Pathology} {Foundation} {Model} for {Whole} {Slide} {Image} {Analysis} via {Vector} {Quantization}},
	shorttitle = {{PathVQ}},
	url = {http://arxiv.org/abs/2503.06482},
	doi = {10.48550/arXiv.2503.06482},
	abstract = {Computational pathology and whole-slide image (WSI) analysis are pivotal in cancer diagnosis and prognosis. However, the ultra-high resolution of WSIs presents significant modeling challenges. Recent advancements in pathology foundation models have improved performance, yet most approaches rely on [CLS] token representation of tile ViT as slide-level inputs (16x16 pixels is refereed as patch and 224x224 pixels as tile). This discards critical spatial details from patch tokens, limiting downstream WSI analysis tasks. We find that leveraging all spatial patch tokens benefits WSI analysis but incurs nearly 200x higher storage and training costs (e.g., 196 tokens in ViT\$\_\{224\}\$). To address this, we introduce vector quantized (VQ) distillation on patch feature, which efficiently compresses spatial patch tokens using discrete indices and a decoder. Our method reduces token dimensionality from 1024 to 16, achieving a 64x compression rate while preserving reconstruction fidelity. Furthermore, we employ a multi-scale VQ (MSVQ) strategy, which not only enhances VQ reconstruction performance but also serves as a Self-supervised Learning (SSL) supervision for a seamless slide-level pretraining objective. Built upon the quantized patch features and supervision targets of tile via MSVQ, we develop a progressive convolutional module and slide-level SSL to extract representations with rich spatial-information for downstream WSI tasks. Extensive evaluations on multiple datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance in WSI analysis. Code will be available soon.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Li, Honglin and Shui, Zhongyi and Zhang, Yunlong and Zhu, Chenglu and Yang, Lin},
	month = mar,
	year = {2025},
	note = {arXiv:2503.06482 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
